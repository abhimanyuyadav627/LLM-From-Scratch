{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZvMxyV5nY0eq0vUwuplWn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhimanyuyadav627/LLM-From-Scratch/blob/main/Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "z-6z6f3EA_F5",
        "outputId": "4c0ebe22-e0d5-4ada-c9e5-ccf30c1735db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is an Embedding ?**\n",
        "* Embedding - **It is a mapping from discrete objects, such as words , images, or even entire documents, to points in a continous vector space.**\n",
        "* While we can be using pretrained word embeddings but it is a common practice for LLMs to produce their own embeddings that are part of input layer and are updated during training."
      ],
      "metadata": {
        "id": "soubNTKKwoxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREPROCESSING STEPS FOR CREATING EMBEDDINGS\n",
        "\n",
        "##### TOKENIZING TEXT\n",
        "\n",
        "Important Considerations:\n",
        "\n",
        "\n",
        "1. When developing a simple tokenizer,\n",
        " whether we should encode whitespaces as seperate characters or just remove them depends on our application and its requirements. **Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example Python code, which is sensitive to indentation and spacing).**\n",
        "\n",
        "2. Adding special tokens - to deal with unknown words that were not a part of vocabulary(not needed with BPE), to deal with situations where we need a seperator for two unrelated text sources.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4kkz4Tfvy21K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "BNB5BHvzCgFa",
        "outputId": "c23dd33b-aa04-44da-b438-ee25e0728ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YkEkmplKvkfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57852198-cbd3-44e2-87f6-85af7d5e1ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "Total number of tokens in the dataset: 5145\n"
          ]
        }
      ],
      "source": [
        "# reading raw text from a text file.\n",
        "with open(\"the-verdict.txt\", \"r\") as f:\n",
        "  raw_text = f.read()\n",
        "encoded_text = tokenizer.encode(raw_text, allowed_special = {\"<|endoftext|>\"})\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(\"Total number of tokens in the dataset:\", len(encoded_text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, Dataloader\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "\n",
        "  def __init__(self,text,tokenizer,max_length,stride):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "\n",
        "    for i in range(0,len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i:i + max_length]\n",
        "      target_chunk = token_ids[i + 1:i + max_length + 1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "tr5w9T_z5TSC",
        "outputId": "242b7148-3e54-4800-c03e-338dcf197c98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.6.0\n"
          ]
        }
      ]
    }
  ]
}